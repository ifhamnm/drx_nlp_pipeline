from sentence_transformers import SentenceTransformer
from llama_cpp import Llama
from vector_db.db_handler import search_vector_db
from utils.logger import log_tokens_per_second
import numpy as np

# Load the LLaMA model (you can tune context length, threads, GPU layers as needed)
llm = Llama(
    model_path="models/llama-2-7b.Q4_K_M.gguf",
    n_ctx=3050,
    n_threads=4,
    n_gpu_layers=2  # Adjust this if you want GPU acceleration
)

# Load the sentence transformer model
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Quick sanity test
response = llm("Q: What is Python?\nA:")
print(response)

# Initialize chat history for context-aware Q&A
chat_history = []

def ask_question(question, top_k=5):
    """
    Handle a user question using Retrieval-Augmented Generation (RAG).

    Args:
        question (str): The user's question.
        top_k (int): Number of top relevant chunks to retrieve from vector DB.

    Returns:
        str: Answer generated by the LLM.
    """
    global chat_history

    # Step 1: Embed the question
    with log_tokens_per_second("üß† Question Embedding"):
        try:
            q_embedding = embedder.encode(question).astype("float32")
        except Exception as e:
            return f"‚ùå Error embedding question: {e}"

    # Step 2: Retrieve top-k relevant context chunks
    relevant_chunks = search_vector_db(q_embedding, top_k=top_k)
    context = "\n\n".join([chunk['text'] for chunk in relevant_chunks])

    # Step 3: Prepare chat history for multi-turn memory
    history_text = "\n".join([f"Q: {q}\nA: {a}" for q, a in chat_history[-3:]])  # Limit to last 3 Q&A pairs

    # Step 4: Construct the LLM prompt
    prompt = f"""
You are a helpful assistant. Use the following research context to answer the user's question as clearly as possible.

{history_text}

--- Context ---
{context}
--- End Context ---

Q: {question}
A:"""

    # Step 5: Call the local LLaMA model
    with log_tokens_per_second("ü§ñ LLaMA Answer"):
        try:
            response = llm(prompt, stop=["Q:"], max_tokens=512)
            answer = response["choices"][0]["text"].strip()
        except Exception as e:
            return f"‚ùå Error from LLaMA: {e}"

    # Step 6: Update history and return
    chat_history.append((question, answer))
    return answer